This repo is for our recent work submitted to bigdata 2024, titled "An Active Learning-Based Streaming Pipeline for Reduced Data Training of Structure Finding Models in Neutron Diffractometry". Here we provide code for the two main contribution of the paper, which are

1). Active learning algorithm
which is about simulating data in a smarter way based on the performance of ML model, so that we can train ML model to the same accuracy with less data.

2). Streaming algorithm
which is about improving the usual serial workflow in active learning using a more resource-friendly workflow that improves the overall performance.

The active learning algorithm can be tested on any systems, and the streaming algorithm requires a job scheduling system. Our code provided here is supposed to
Support 1) on any system.
Support 2) only on Polaris and Perlmutter. 

To be able to run the code and reproduce the results of the paper, please following the following steps:

Step 1: Install environment
1a). Install packages for training and active learning tasks:

On Perlmutter:

module load pytorch/2.0.1

On Polaris:

module use /soft/modulefiles
module load conda/2024-04-29

On other system, we provide a packages list. A shifter container will be provided shortly to alleviate the need for building environment from scratch. Before that, user can use the following command to build env:

conda env create -f environment.yml -n <env_name>

1b). Install GSAS-II for simulation tasks:

g2="https://github.com/AdvancedPhotonSource/GSAS-II-buildtools/releases/download/v1.0.1/gsas2full-Latest-Linux-x86_64.sh"
curl -L "$g2" > /tmp/g2.sh
bash /tmp/g2.sh -b -p ~/g2full

Please note that, if you don't want to install into ~/g2full, please search and replace that string in executable directory and make corresponding changes.


Step 2: Running the baseline

In this step we try to perform baseline experiment and sweep over multiple dataset size. We also run with multiple random number seed for robustness. This will give us the black error bar in Figure 7,8. Note: We will not get exactly the same number, but results will be consistent.

The command for running this experiment will be:

cd workflow
qsub submit_baseline.sh 


Before executing this command, modify the script according to the following directions:

Here the script will be executed multiple times, with the following parameter combination:

seed (line 12): 13010, 13110, 13210, 13310, 13410, 13510
num_sample (line 18): 40000, 80000, 120000, 160000, 200000, 240000
In total 6*6=36 experiments shall be done to generate six data point for baseline experiment with error bar

Before running real executable, need to setup env (See step 1). Modify line 9 accordingly.

Setting up the work_dir as the dir where this repo is. Change line 13 accordingly.

Also notice, for num_sample that exceeds 120000, task can not finish within one hour, need to use different queue (like preemptable) on Polaris


Step 3: Running the serial workflow

In this step we try to run the active learning serial workflow. This will output two important data: a). Accuracy performance of Active learning, and b). Running time performance of serial workflow. This will give us the black/red error band in Figure 7,8, data associate with serial workflow in Figure 9,10 and Table III,IV,V. Note: We will not get exactly the same number, but results will be consistent.

The command for running this experiment will be 

cd workflow
qsub submit_serial.sh


Before executing this command, modify the script according to the following directions:

Use your own project name (line 7)

To get the red and blue error band in Figure 7,8, data associated with serial workflow in Figure 9,10 and Table III (i.e., Experiment E1), use the following parameter combination:
seed (line 12): 21000, 21100, 21200, 21300, 21400, 21500
num_sample (line 18): 4500
batch_size (line 23): 512

To get data associated with serial workflow in Table IV,V (i.e., Experiment E2), use the following parameter combination:
number_of_nodes (line 2): 1, 2, 4
queue (line 6): preemptable
walltime (line 4): 12:00:00
seed (line 12): 31000
num_sample (line 18): 72000
batch_size (line 23): 2048
